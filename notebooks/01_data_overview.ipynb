{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Overview - Clinical Deterioration Prediction\n",
        "\n",
        "This notebook provides an initial overview and comparison of the two datasets:\n",
        "- **3-year dataset** (CNUH_3Y.csv): ~317k rows\n",
        "- **10-year dataset** (10yrs_proc.csv): ~38k rows\n",
        "\n",
        "## Objectives\n",
        "1. Load and validate both datasets\n",
        "2. Compare dataset structures\n",
        "3. Initial data quality assessment\n",
        "4. Feature mapping between datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from src.data_loader import (\n",
        "    load_3year_dataset, \n",
        "    load_10year_dataset, \n",
        "    validate_dataset, \n",
        "    get_feature_groups\n",
        ")\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load 3-year dataset\n",
        "print(\"Loading 3-year dataset...\")\n",
        "df_3year = load_3year_dataset(\"../data/CNUH_3Y.csv\")\n",
        "print(f\"✓ Loaded: {df_3year.shape[0]:,} rows × {df_3year.shape[1]} columns\")\n",
        "\n",
        "# Load 10-year dataset\n",
        "print(\"\\nLoading 10-year dataset...\")\n",
        "df_10year = load_10year_dataset(\"../data/10yrs_proc.csv\")\n",
        "print(f\"✓ Loaded: {df_10year.shape[0]:,} rows × {df_10year.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Structure Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare column names\n",
        "cols_3year = set(df_3year.columns)\n",
        "cols_10year = set(df_10year.columns)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COLUMN COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n3-year dataset columns: {len(cols_3year)}\")\n",
        "print(f\"10-year dataset columns: {len(cols_10year)}\")\n",
        "print(f\"\\nCommon columns: {len(cols_3year & cols_10year)}\")\n",
        "print(f\"Unique to 3-year: {len(cols_3year - cols_10year)}\")\n",
        "print(f\"Unique to 10-year: {len(cols_10year - cols_3year)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Columns unique to 3-year dataset:\")\n",
        "print(\"-\" * 60)\n",
        "for col in sorted(cols_3year - cols_10year):\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Columns unique to 10-year dataset:\")\n",
        "print(\"-\" * 60)\n",
        "for col in sorted(cols_10year - cols_3year):\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Common columns (first 10):\")\n",
        "print(\"-\" * 60)\n",
        "common_cols = sorted(cols_3year & cols_10year)\n",
        "for col in common_cols[:10]:\n",
        "    print(f\"  - {col}\")\n",
        "if len(common_cols) > 10:\n",
        "    print(f\"  ... and {len(common_cols) - 10} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate 3-year dataset\n",
        "print(\"=\" * 60)\n",
        "print(\"3-YEAR DATASET VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "val_3year = validate_dataset(df_3year, \"3-year dataset\")\n",
        "print(f\"Shape: {val_3year['shape']}\")\n",
        "print(f\"Memory usage: {val_3year['memory_usage_mb']:.2f} MB\")\n",
        "print(f\"Duplicate rows: {val_3year['duplicate_rows']:,}\")\n",
        "\n",
        "if 'target_distribution' in val_3year:\n",
        "    print(f\"\\nTarget distribution:\")\n",
        "    for k, v in val_3year['target_distribution'].items():\n",
        "        pct = v / val_3year['shape'][0] * 100\n",
        "        print(f\"  Class {k}: {v:,} ({pct:.2f}%)\")\n",
        "\n",
        "print(f\"\\nMissing values (top 10):\")\n",
        "missing_3year = pd.Series(val_3year['missing_percentage']).sort_values(ascending=False)\n",
        "for col, pct in missing_3year.head(10).items():\n",
        "    if pct > 0:\n",
        "        print(f\"  {col}: {pct:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate 10-year dataset\n",
        "print(\"=\" * 60)\n",
        "print(\"10-YEAR DATASET VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "val_10year = validate_dataset(df_10year, \"10-year dataset\")\n",
        "print(f\"Shape: {val_10year['shape']}\")\n",
        "print(f\"Memory usage: {val_10year['memory_usage_mb']:.2f} MB\")\n",
        "print(f\"Duplicate rows: {val_10year['duplicate_rows']:,}\")\n",
        "\n",
        "if 'target_distribution' in val_10year:\n",
        "    print(f\"\\nTarget distribution:\")\n",
        "    for k, v in val_10year['target_distribution'].items():\n",
        "        pct = v / val_10year['shape'][0] * 100\n",
        "        print(f\"  Class {k}: {v:,} ({pct:.2f}%)\")\n",
        "\n",
        "print(f\"\\nMissing values (top 10):\")\n",
        "missing_10year = pd.Series(val_10year['missing_percentage']).sort_values(ascending=False)\n",
        "for col, pct in missing_10year.head(10).items():\n",
        "    if pct > 0:\n",
        "        print(f\"  {col}: {pct:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Groups Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature groups for both datasets\n",
        "features_3year = get_feature_groups(df_3year)\n",
        "features_10year = get_feature_groups(df_10year)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE GROUPS - 3-YEAR DATASET\")\n",
        "print(\"=\" * 60)\n",
        "for group, features in features_3year.items():\n",
        "    if features:\n",
        "        print(f\"\\n{group.upper().replace('_', ' ')} ({len(features)} features):\")\n",
        "        for feat in features:\n",
        "            print(f\"  - {feat}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURE GROUPS - 10-YEAR DATASET\")\n",
        "print(\"=\" * 60)\n",
        "for group, features in features_10year.items():\n",
        "    if features:\n",
        "        print(f\"\\n{group.upper().replace('_', ' ')} ({len(features)} features):\")\n",
        "        for feat in features:\n",
        "            print(f\"  - {feat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sample Data Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"3-YEAR DATASET - First 5 rows:\")\n",
        "print(\"=\" * 60)\n",
        "display(df_3year.head())\n",
        "\n",
        "print(\"\\n3-YEAR DATASET - Basic Statistics:\")\n",
        "print(\"=\" * 60)\n",
        "display(df_3year.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"10-YEAR DATASET - First 5 rows:\")\n",
        "print(\"=\" * 60)\n",
        "display(df_10year.head())\n",
        "\n",
        "print(\"\\n10-YEAR DATASET - Basic Statistics:\")\n",
        "print(\"=\" * 60)\n",
        "display(df_10year.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Key Findings\n",
        "\n",
        "### Dataset Comparison Summary:\n",
        "1. **Size**: 3-year dataset is ~8x larger (317k vs 38k rows)\n",
        "2. **Features**: Both datasets contain similar clinical features (vital signs, lab values)\n",
        "3. **Target**: Both have binary target variable (0 = normal, 1 = deterioration)\n",
        "4. **Temporal**: Both include time-based features for tracking patient measurements\n",
        "\n",
        "### Next Steps:\n",
        "- Detailed EDA for each dataset\n",
        "- Target variable analysis\n",
        "- Feature importance analysis\n",
        "- Problem definition"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
